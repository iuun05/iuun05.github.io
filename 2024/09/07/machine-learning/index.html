<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>机器学习作业 | </title><meta name="author" content="LiamRyan"><meta name="copyright" content="LiamRyan"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="week 2 PROBLEM 1		Regression on Salary 123456789101112131. Description. As learned in class, you want to analyze what factors would affect the salary of graduates’ first jobs, and then you collect 500">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习作业">
<meta property="og:url" content="http://iuun05.github.io/2024/09/07/machine-learning/index.html">
<meta property="og:site_name">
<meta property="og:description" content="week 2 PROBLEM 1		Regression on Salary 123456789101112131. Description. As learned in class, you want to analyze what factors would affect the salary of graduates’ first jobs, and then you collect 500">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://iuun05.github.io/img/20220513011040_fc477.png">
<meta property="article:published_time" content="2024-09-06T17:52:24.000Z">
<meta property="article:modified_time" content="2025-07-26T10:33:42.397Z">
<meta property="article:author" content="LiamRyan">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://iuun05.github.io/img/20220513011040_fc477.png"><link rel="shortcut icon" href="/GWdBd10WcAAyRuc.jpg"><link rel="canonical" href="http://iuun05.github.io/2024/09/07/machine-learning/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.0.0-b1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.6.0/css/all.min.css"><script>(()=>{
      const saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
      
      window.btf = {
        saveToLocal: saveToLocal,
        getScript: (url, attr = {}) => new Promise((resolve, reject) => {
          const script = document.createElement('script')
          script.src = url
          script.async = true
          script.onerror = reject
          script.onload = script.onreadystatechange = function() {
            const loadState = this.readyState
            if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
            script.onload = script.onreadystatechange = null
            resolve()
          }

          Object.keys(attr).forEach(key => {
            script.setAttribute(key, attr[key])
          })

          document.head.appendChild(script)
        }),

        getCSS: (url, id = false) => new Promise((resolve, reject) => {
          const link = document.createElement('link')
          link.rel = 'stylesheet'
          link.href = url
          if (id) link.id = id
          link.onerror = reject
          link.onload = link.onreadystatechange = function() {
            const loadState = this.readyState
            if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
            link.onload = link.onreadystatechange = null
            resolve()
          }
          document.head.appendChild(link)
        }),

        addGlobalFn: (key, fn, name = false, parent = window) => {
          const pjaxEnable = false
          if (!pjaxEnable && key.startsWith('pjax')) return

          const globalFn = parent.globalFn || {}
          const keyObj = globalFn[key] || {}
    
          if (name && keyObj[name]) return
    
          name = name || Object.keys(keyObj).length
          keyObj[name] = fn
          globalFn[key] = keyObj
          parent.globalFn = globalFn
        }
      }
    
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode
      
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })()</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.12.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '机器学习作业',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-07-26 18:33:42'
}</script><meta name="generator" content="Hexo 7.3.0"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/20220513011040_fc477.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">8</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title=""><img class="site-icon" src="/img/20220513011040_fc477.png" alt="Logo"/><span class="site-name"></span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">机器学习作业</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-09-06T17:52:24.000Z" title="发表于 2024-09-07 01:52:24">2024-09-07</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-07-26T10:33:42.397Z" title="更新于 2025-07-26 18:33:42">2025-07-26</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">5.5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>30分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="机器学习作业"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1>week 2</h1>
<h2 id="problem-1-regression-on-salary">PROBLEM 1		Regression on Salary</h2>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">1. Description. As learned in class, you want to analyze what factors would affect the salary of graduates’ first jobs, and then you collect 500 graduates’ data for analysis. See the “salary.csv” file attached. There are 5 features (GPA, # of internships, # of Competitions, # of Penalties, and Nationality) and 1 label (Salary). Use a linear regression model to analyze this dataset.</span><br><span class="line"></span><br><span class="line">2. Visualization. The very first step is to familiarize yourself with the data. You will need to make 6 plots. The first is a salary histogram, helping you understand its distribution. Then, draw feature-label scattered plots for each feature vs. label to see their correlations (5 features, so 5 plots). Briefly state your findings from these plots.</span><br><span class="line"></span><br><span class="line">3. K-fold cross validation. Use K=5. Evenly partition the dataset into 5 chunks, denoted as C1, C2, C3, C4, C5, and do 5 runs of experiments.</span><br><span class="line">For each run i = 1, 2, 3, 4, 5:</span><br><span class="line">	Use Ci as test data and the rest as training data;</span><br><span class="line">	Data preprocessing. Standardize the data for each feature and label. Note for continuous columns such as GPA and Salary, only use training data to compute the sample mean and variance, and use them to normalize both training data and test data.</span><br><span class="line">	Use Least Square to find the best model weights on training data, and evaluate the model on test data. Report the model weights, training MSE, and test MSE.</span><br><span class="line">After all the five runs, use a bar chart to visualize the five runs’ training MSE and test MSE, and report an averaged MSE ± 1 standard deviation.</span><br><span class="line"></span><br><span class="line">4. Draw conclusions. Based on your model results, briefly describe the experiment results, analyze the results, and draw conclusions.</span><br><span class="line">5. Report. You will need to compose a report containing the abovementioned figures, observations, analysis, and conclusions. Try to write it as professionally as you can. Make it concise and elegant. </span><br></pre></td></tr></table></figure>
<h3 id="report">report</h3>
<pre><code>略
</code></pre>
<h3 id="source-code">source code</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br><span class="line">655</span><br><span class="line">656</span><br><span class="line">657</span><br><span class="line">658</span><br><span class="line">659</span><br><span class="line">660</span><br><span class="line">661</span><br><span class="line">662</span><br><span class="line">663</span><br><span class="line">664</span><br><span class="line">665</span><br><span class="line">666</span><br><span class="line">667</span><br><span class="line">668</span><br><span class="line">669</span><br><span class="line">670</span><br><span class="line">671</span><br><span class="line">672</span><br><span class="line">673</span><br><span class="line">674</span><br><span class="line">675</span><br><span class="line">676</span><br><span class="line">677</span><br><span class="line">678</span><br><span class="line">679</span><br><span class="line">680</span><br><span class="line">681</span><br><span class="line">682</span><br><span class="line">683</span><br><span class="line">684</span><br><span class="line">685</span><br><span class="line">686</span><br><span class="line">687</span><br><span class="line">688</span><br><span class="line">689</span><br><span class="line">690</span><br><span class="line">691</span><br><span class="line">692</span><br><span class="line">693</span><br><span class="line">694</span><br><span class="line">695</span><br><span class="line">696</span><br><span class="line">697</span><br><span class="line">698</span><br><span class="line">699</span><br><span class="line">700</span><br><span class="line">701</span><br><span class="line">702</span><br><span class="line">703</span><br><span class="line">704</span><br><span class="line">705</span><br><span class="line">706</span><br><span class="line">707</span><br><span class="line">708</span><br><span class="line">709</span><br><span class="line">710</span><br><span class="line">711</span><br><span class="line">712</span><br><span class="line">713</span><br><span class="line">714</span><br><span class="line">715</span><br><span class="line">716</span><br><span class="line">717</span><br><span class="line">718</span><br><span class="line">719</span><br><span class="line">720</span><br><span class="line">721</span><br><span class="line">722</span><br><span class="line">723</span><br><span class="line">724</span><br><span class="line">725</span><br><span class="line">726</span><br><span class="line">727</span><br><span class="line">728</span><br><span class="line">729</span><br><span class="line">730</span><br><span class="line">731</span><br><span class="line">732</span><br><span class="line">733</span><br><span class="line">734</span><br><span class="line">735</span><br><span class="line">736</span><br><span class="line">737</span><br><span class="line">738</span><br><span class="line">739</span><br><span class="line">740</span><br><span class="line">741</span><br><span class="line">742</span><br><span class="line">743</span><br><span class="line">744</span><br><span class="line">745</span><br><span class="line">746</span><br><span class="line">747</span><br><span class="line">748</span><br><span class="line">749</span><br><span class="line">750</span><br><span class="line">751</span><br><span class="line">752</span><br><span class="line">753</span><br><span class="line">754</span><br><span class="line">755</span><br><span class="line">756</span><br><span class="line">757</span><br><span class="line">758</span><br><span class="line">759</span><br><span class="line">760</span><br><span class="line">761</span><br><span class="line">762</span><br><span class="line">763</span><br><span class="line">764</span><br><span class="line">765</span><br><span class="line">766</span><br><span class="line">767</span><br><span class="line">768</span><br><span class="line">769</span><br><span class="line">770</span><br><span class="line">771</span><br><span class="line">772</span><br><span class="line">773</span><br><span class="line">774</span><br><span class="line">775</span><br><span class="line">776</span><br><span class="line">777</span><br><span class="line">778</span><br><span class="line">779</span><br><span class="line">780</span><br><span class="line">781</span><br><span class="line">782</span><br><span class="line">783</span><br><span class="line">784</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为了和自己计算的数据进行对比,只写了作业1的K-fold</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> graphviz <span class="keyword">import</span> Digraph</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> export_graphviz, DecisionTreeClassifier</span><br><span class="line"><span class="keyword">import</span> graphviz</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">dataprocess</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data_path</span>):</span><br><span class="line">        <span class="variable language_">self</span>.threshold = <span class="number">130000</span></span><br><span class="line">        <span class="variable language_">self</span>.data = pd.read_csv(data_path).values</span><br><span class="line">        <span class="variable language_">self</span>.GPA = np.array(<span class="variable language_">self</span>.data[:, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将薪资表的数据读取出来，并且进行分割</span></span><br><span class="line">        <span class="variable language_">self</span>.Internships = np.array(<span class="variable language_">self</span>.data[:, <span class="number">1</span>])</span><br><span class="line">        <span class="variable language_">self</span>.Competitions = np.array(<span class="variable language_">self</span>.data[:, <span class="number">2</span>])</span><br><span class="line">        <span class="variable language_">self</span>.Penalties = np.array(<span class="variable language_">self</span>.data[:, <span class="number">3</span>])</span><br><span class="line">        <span class="variable language_">self</span>.Nationality = np.array(<span class="variable language_">self</span>.data[:, <span class="number">4</span>])</span><br><span class="line">        <span class="variable language_">self</span>.Salary = np.array(<span class="variable language_">self</span>.data[:, <span class="number">5</span>])</span><br><span class="line">        <span class="comment"># 将工资大于阈值的标记为1，小于的标记为0</span></span><br><span class="line">        <span class="variable language_">self</span>.Salary_Classification = np.where(<span class="variable language_">self</span>.Salary &gt; <span class="variable language_">self</span>.threshold, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">        <span class="variable language_">self</span>.C = np.array([<span class="variable language_">self</span>.data[<span class="number">100</span> * i:<span class="number">100</span> * (i + <span class="number">1</span>), :] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>)])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将国家数据转换为one-hot编码</span></span><br><span class="line">        <span class="variable language_">self</span>.Nationality_onehot = <span class="variable language_">self</span>.one_hot_encoder(<span class="variable language_">self</span>.Nationality)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.batchs = [<span class="number">1</span>, <span class="number">4</span>, <span class="number">8</span>, <span class="number">16</span>]</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.weights = <span class="literal">None</span></span><br><span class="line">        <span class="variable language_">self</span>.feature = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.beta = <span class="literal">None</span>  <span class="comment"># 存储训练后的权重</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#数据可视化操作</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">Visualization</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 薪资统计</span></span><br><span class="line">        fig = plt.figure(figsize=(<span class="number">16</span>, <span class="number">12</span>))</span><br><span class="line">        ax1 = fig.add_subplot(<span class="number">221</span>)</span><br><span class="line">        ax1.hist(<span class="variable language_">self</span>.Salary, bins=<span class="number">15</span>, color=<span class="string">&#x27;#f59311&#x27;</span>, alpha=<span class="number">0.5</span>, edgecolor=<span class="string">&#x27;k&#x27;</span>)</span><br><span class="line">        ax1.set_title(<span class="string">&#x27;Salary&#x27;</span>)</span><br><span class="line">        ax1.set_xlabel(<span class="string">&#x27;Salary&#x27;</span>)</span><br><span class="line">        ax1.set_ylabel(<span class="string">&#x27;Frequency&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将GPA、实习、竞赛、罚款与薪资的关系可视化</span></span><br><span class="line">        ax2 = fig.add_subplot(<span class="number">222</span>)</span><br><span class="line">        ax2.scatter(<span class="variable language_">self</span>.GPA, <span class="variable language_">self</span>.Salary, color=<span class="string">&#x27;#f59311&#x27;</span>, alpha=<span class="number">0.5</span>, edgecolor=<span class="string">&#x27;k&#x27;</span>)</span><br><span class="line">        ax2.set_title(<span class="string">&#x27;GPA vs Salary&#x27;</span>)</span><br><span class="line">        ax2.set_xlabel(<span class="string">&#x27;GPA&#x27;</span>)</span><br><span class="line">        ax2.set_ylabel(<span class="string">&#x27;Salary&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        ax3 = fig.add_subplot(<span class="number">223</span>)</span><br><span class="line">        ax3.scatter(<span class="variable language_">self</span>.Internships, <span class="variable language_">self</span>.Salary, color=<span class="string">&#x27;#f59311&#x27;</span>, alpha=<span class="number">0.5</span>, edgecolor=<span class="string">&#x27;k&#x27;</span>)</span><br><span class="line">        ax3.set_title(<span class="string">&#x27;Internships vs Salary&#x27;</span>)</span><br><span class="line">        ax3.set_xlabel(<span class="string">&#x27;Internships&#x27;</span>)</span><br><span class="line">        ax3.set_ylabel(<span class="string">&#x27;Salary&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        ax4 = fig.add_subplot(<span class="number">224</span>)</span><br><span class="line">        ax4.scatter(<span class="variable language_">self</span>.Competitions, <span class="variable language_">self</span>.Salary, color=<span class="string">&#x27;#f59311&#x27;</span>, alpha=<span class="number">0.5</span>, edgecolor=<span class="string">&#x27;k&#x27;</span>)</span><br><span class="line">        ax4.set_title(<span class="string">&#x27;Competitions vs Salary&#x27;</span>)</span><br><span class="line">        ax4.set_xlabel(<span class="string">&#x27;Competitions&#x27;</span>)</span><br><span class="line">        ax4.set_ylabel(<span class="string">&#x27;Salary&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        plt.show()</span><br><span class="line"></span><br><span class="line">        fig = plt.figure(figsize=(<span class="number">16</span>, <span class="number">12</span>))</span><br><span class="line">        ax5 = fig.add_subplot(<span class="number">231</span>)</span><br><span class="line">        ax5.scatter(<span class="variable language_">self</span>.Penalties, <span class="variable language_">self</span>.Salary, color=<span class="string">&#x27;#f59311&#x27;</span>, alpha=<span class="number">0.5</span>, edgecolor=<span class="string">&#x27;k&#x27;</span>)</span><br><span class="line">        ax5.set_title(<span class="string">&#x27;Penalties vs Salary&#x27;</span>)</span><br><span class="line">        ax5.set_xlabel(<span class="string">&#x27;Penalties&#x27;</span>)</span><br><span class="line">        ax5.set_ylabel(<span class="string">&#x27;Salary&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        ax6 = fig.add_subplot(<span class="number">232</span>)</span><br><span class="line">        ax6.scatter(<span class="variable language_">self</span>.Nationality, <span class="variable language_">self</span>.Salary, color=<span class="string">&#x27;#f59311&#x27;</span>, alpha=<span class="number">0.5</span>, edgecolor=<span class="string">&#x27;k&#x27;</span>)</span><br><span class="line">        ax6.set_title(<span class="string">&#x27;Nationality vs Salary&#x27;</span>)</span><br><span class="line">        ax6.set_xlabel(<span class="string">&#x27;Nationality&#x27;</span>)</span><br><span class="line">        ax6.set_ylabel(<span class="string">&#x27;Salary&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算均值和方差</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">average</span>(<span class="params">self, C, K=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># data = np.array([])</span></span><br><span class="line">        <span class="comment"># for i in range(5):</span></span><br><span class="line">        <span class="comment">#     if i is not K:</span></span><br><span class="line">        <span class="comment">#         data = np.hstack((data, C[100*i:100*(i+1)]))</span></span><br><span class="line">        <span class="keyword">return</span> np.mean(C, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">variance</span>(<span class="params">self, C, K=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># data = np.array([])</span></span><br><span class="line">        <span class="comment"># for i in range(5):</span></span><br><span class="line">        <span class="comment">#     if i is not K:</span></span><br><span class="line">        <span class="comment">#         data = np.hstack((data, C[100 * i:100 * (i + 1)]))</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> np.var(C, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># One-Hot 编码</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">one_hot_encoder</span>(<span class="params">self, Nationality</span>):</span><br><span class="line"></span><br><span class="line">        unique_categories = np.unique(Nationality)</span><br><span class="line"></span><br><span class="line">        category_to_index = &#123;category: index <span class="keyword">for</span> index, category <span class="keyword">in</span> <span class="built_in">enumerate</span>(unique_categories)&#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化 One-Hot 编码矩阵</span></span><br><span class="line">        Nationality_onehot = np.zeros((<span class="built_in">len</span>(Nationality), <span class="built_in">len</span>(unique_categories)))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将类别映射为 One-Hot 向量</span></span><br><span class="line">        <span class="keyword">for</span> i, category <span class="keyword">in</span> <span class="built_in">enumerate</span>(Nationality):</span><br><span class="line">            Nationality_onehot[i, category_to_index[category]] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># print(&quot;Unique categories:&quot;, unique_categories)</span></span><br><span class="line">        <span class="comment"># print(&quot;One-Hot Encoded matrix:&quot;)</span></span><br><span class="line">        <span class="comment"># print(self.Nationality_onehot)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> Nationality_onehot</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将连续数据标准化</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">standardization</span>(<span class="params">self, C, average, variance, k</span>):</span><br><span class="line">        <span class="keyword">return</span> (C - average) / variance</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 最小二乘法</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">Least_Square</span>(<span class="params">self, train_data, train_label, test_data, test_label</span>):</span><br><span class="line">        X_transpose = np.transpose(train_data)</span><br><span class="line"></span><br><span class="line">        XTX = np.dot(X_transpose, train_data)</span><br><span class="line"></span><br><span class="line">        XTX_inv = np.linalg.inv(XTX)</span><br><span class="line"></span><br><span class="line">        XTy = np.dot(X_transpose, train_label)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.beta = np.dot(XTX_inv, XTy)</span><br><span class="line"></span><br><span class="line">        train_predictions = np.dot(train_data, <span class="variable language_">self</span>.beta)</span><br><span class="line">        test_predictions = np.dot(test_data, <span class="variable language_">self</span>.beta)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算训练数据和测试数据的均方误差（MSE）</span></span><br><span class="line">        train_mse = np.mean((train_predictions - train_label) ** <span class="number">2</span>)</span><br><span class="line">        test_mse = np.mean((test_predictions - test_label) ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># print(&quot;Model weights (beta):&quot;, self.beta)</span></span><br><span class="line">        <span class="comment"># print(&quot;Training MSE:&quot;, train_mse)</span></span><br><span class="line">        <span class="comment"># print(&quot;Test MSE:&quot;, test_mse)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.beta, train_mse, test_mse</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对任务3的二分类数据进行处理</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">task3_dataprocess</span>(<span class="params">self, i</span>):</span><br><span class="line">        <span class="comment"># 数据归一化，以及相关的划分</span></span><br><span class="line">        <span class="comment"># 除薪资数据外，其余部分与 k-fold 中数据处理方法大致相同</span></span><br><span class="line">        GPA_average = <span class="variable language_">self</span>.average(np.delete(<span class="variable language_">self</span>.GPA, <span class="built_in">slice</span>(<span class="number">100</span> * i, <span class="number">100</span> * (i + <span class="number">1</span>))), i)</span><br><span class="line">        GPA_variance = <span class="variable language_">self</span>.variance(np.delete(<span class="variable language_">self</span>.GPA, <span class="built_in">slice</span>(<span class="number">100</span> * i, <span class="number">100</span> * (i + <span class="number">1</span>))), i)</span><br><span class="line">        GPA = <span class="variable language_">self</span>.standardization(<span class="variable language_">self</span>.GPA, GPA_average, GPA_variance, i)</span><br><span class="line"></span><br><span class="line">        GPA_test = GPA[<span class="number">100</span> * i:<span class="number">100</span> * (i + <span class="number">1</span>)]</span><br><span class="line">        GPA = np.delete(GPA, <span class="built_in">slice</span>(<span class="number">100</span> * i, <span class="number">100</span> * (i + <span class="number">1</span>)), axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        Interships_test = <span class="variable language_">self</span>.Internships[<span class="number">100</span> * i:<span class="number">100</span> * (i + <span class="number">1</span>)]</span><br><span class="line">        Interships = np.delete(<span class="variable language_">self</span>.Internships, <span class="built_in">slice</span>(<span class="number">100</span> * i, <span class="number">100</span> * (i + <span class="number">1</span>)), axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        Competitions_test = <span class="variable language_">self</span>.Competitions[<span class="number">100</span> * i:<span class="number">100</span> * (i + <span class="number">1</span>)]</span><br><span class="line">        Competitions = np.delete(<span class="variable language_">self</span>.Competitions, <span class="built_in">slice</span>(<span class="number">100</span> * i, <span class="number">100</span> * (i + <span class="number">1</span>)), axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        Penalties_test = <span class="variable language_">self</span>.Penalties[<span class="number">100</span> * i:<span class="number">100</span> * (i + <span class="number">1</span>)]</span><br><span class="line">        Penalties = np.delete(<span class="variable language_">self</span>.Penalties, <span class="built_in">slice</span>(<span class="number">100</span> * i, <span class="number">100</span> * (i + <span class="number">1</span>)), axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        Nationality_onehot = <span class="variable language_">self</span>.Nationality_onehot</span><br><span class="line">        Nationality_test = Nationality_onehot[<span class="number">100</span> * i:<span class="number">100</span> * (i + <span class="number">1</span>)]</span><br><span class="line">        Nationality = np.delete(Nationality_onehot, <span class="built_in">slice</span>(<span class="number">100</span> * i, <span class="number">100</span> * (i + <span class="number">1</span>)), axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Salary_average = self.average(np.delete(self.Salary, slice(100 * i, 100 * (i + 1))), i)</span></span><br><span class="line">        <span class="comment"># Salary_variance = self.variance(np.delete(self.Salary, slice(100 * i, 100 * (i + 1))), i)</span></span><br><span class="line">        <span class="comment"># Salary = self.standardization(self.Salary, Salary_average, Salary_variance, i)</span></span><br><span class="line">        <span class="comment"># high = 1, low = 0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将薪资的数据进行比较替换</span></span><br><span class="line">        Salary = <span class="variable language_">self</span>.Salary_Classification</span><br><span class="line">        <span class="comment"># print(Salary)</span></span><br><span class="line">        <span class="comment"># print(Salary.shape)</span></span><br><span class="line">        Salary_test = Salary[<span class="number">100</span> * i:<span class="number">100</span> * (i + <span class="number">1</span>)]</span><br><span class="line">        Salary = np.delete(Salary, <span class="built_in">slice</span>(<span class="number">100</span> * i, <span class="number">100</span> * (i + <span class="number">1</span>)), axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># train_data = np.hstack(GPA, Interships, Competitions, Penalties, Nationality)</span></span><br><span class="line">        train_data = np.concatenate((GPA[:, np.newaxis], Interships[:, np.newaxis], Competitions[:, np.newaxis],</span><br><span class="line">                                     Penalties[:, np.newaxis], Nationality), axis=<span class="number">1</span>).astype(<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line">        train_label = Salary.astype(<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line">        test_data = np.concatenate(</span><br><span class="line">            (GPA_test[:, np.newaxis], Interships_test[:, np.newaxis], Competitions_test[:, np.newaxis],</span><br><span class="line">             Penalties_test[:, np.newaxis], Nationality_test), axis=<span class="number">1</span>).astype(<span class="built_in">float</span>)</span><br><span class="line">        test_label = Salary_test.astype(<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 返回分好的训练数据和测试数据</span></span><br><span class="line">        <span class="keyword">return</span> train_data, train_label, test_data, test_label</span><br><span class="line"></span><br><span class="line">    <span class="comment"># i 表述第几个数据集</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">pre_dataprocess</span>(<span class="params">self, i</span>):</span><br><span class="line">        <span class="comment"># 数据归一化，以及相关的划分</span></span><br><span class="line">        GPA_average = <span class="variable language_">self</span>.average(np.delete(<span class="variable language_">self</span>.GPA, <span class="built_in">slice</span>(<span class="number">100</span> * i, <span class="number">100</span> * (i + <span class="number">1</span>))), i)</span><br><span class="line">        GPA_variance = <span class="variable language_">self</span>.variance(np.delete(<span class="variable language_">self</span>.GPA, <span class="built_in">slice</span>(<span class="number">100</span> * i, <span class="number">100</span> * (i + <span class="number">1</span>))), i)</span><br><span class="line">        GPA = <span class="variable language_">self</span>.standardization(<span class="variable language_">self</span>.GPA, GPA_average, GPA_variance, i)</span><br><span class="line"></span><br><span class="line">        GPA_test = GPA[<span class="number">100</span> * i:<span class="number">100</span> * (i + <span class="number">1</span>)]</span><br><span class="line">        GPA = np.delete(GPA, <span class="built_in">slice</span>(<span class="number">100</span> * i, <span class="number">100</span> * (i + <span class="number">1</span>)), axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        Interships_test = <span class="variable language_">self</span>.Internships[<span class="number">100</span> * i:<span class="number">100</span> * (i + <span class="number">1</span>)]</span><br><span class="line">        Interships = np.delete(<span class="variable language_">self</span>.Internships, <span class="built_in">slice</span>(<span class="number">100</span> * i, <span class="number">100</span> * (i + <span class="number">1</span>)), axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        Competitions_test = <span class="variable language_">self</span>.Competitions[<span class="number">100</span> * i:<span class="number">100</span> * (i + <span class="number">1</span>)]</span><br><span class="line">        Competitions = np.delete(<span class="variable language_">self</span>.Competitions, <span class="built_in">slice</span>(<span class="number">100</span> * i, <span class="number">100</span> * (i + <span class="number">1</span>)), axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        Penalties_test = <span class="variable language_">self</span>.Penalties[<span class="number">100</span> * i:<span class="number">100</span> * (i + <span class="number">1</span>)]</span><br><span class="line">        Penalties = np.delete(<span class="variable language_">self</span>.Penalties, <span class="built_in">slice</span>(<span class="number">100</span> * i, <span class="number">100</span> * (i + <span class="number">1</span>)), axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        Nationality_onehot = <span class="variable language_">self</span>.Nationality_onehot</span><br><span class="line">        Nationality_test = Nationality_onehot[<span class="number">100</span> * i:<span class="number">100</span> * (i + <span class="number">1</span>)]</span><br><span class="line">        Nationality = np.delete(Nationality_onehot, <span class="built_in">slice</span>(<span class="number">100</span> * i, <span class="number">100</span> * (i + <span class="number">1</span>)), axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        Salary_average = <span class="variable language_">self</span>.average(np.delete(<span class="variable language_">self</span>.Salary, <span class="built_in">slice</span>(<span class="number">100</span> * i, <span class="number">100</span> * (i + <span class="number">1</span>))), i)</span><br><span class="line">        Salary_variance = <span class="variable language_">self</span>.variance(np.delete(<span class="variable language_">self</span>.Salary, <span class="built_in">slice</span>(<span class="number">100</span> * i, <span class="number">100</span> * (i + <span class="number">1</span>))), i)</span><br><span class="line">        Salary = <span class="variable language_">self</span>.standardization(<span class="variable language_">self</span>.Salary, Salary_average, Salary_variance, i)</span><br><span class="line"></span><br><span class="line">        Salary_test = Salary[<span class="number">100</span> * i:<span class="number">100</span> * (i + <span class="number">1</span>)]</span><br><span class="line">        Salary = np.delete(Salary, <span class="built_in">slice</span>(<span class="number">100</span> * i, <span class="number">100</span> * (i + <span class="number">1</span>)), axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># train_data = np.hstack(GPA, Interships, Competitions, Penalties, Nationality)</span></span><br><span class="line">        train_data = np.concatenate((GPA[:, np.newaxis], Interships[:, np.newaxis], Competitions[:, np.newaxis],</span><br><span class="line">                                     Penalties[:, np.newaxis], Nationality), axis=<span class="number">1</span>).astype(<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line">        train_label = Salary.astype(<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line">        test_data = np.concatenate(</span><br><span class="line">            (GPA_test[:, np.newaxis], Interships_test[:, np.newaxis], Competitions_test[:, np.newaxis],</span><br><span class="line">             Penalties_test[:, np.newaxis], Nationality_test), axis=<span class="number">1</span>).astype(<span class="built_in">float</span>)</span><br><span class="line">        test_label = Salary_test.astype(<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> train_data, train_label, test_data, test_label</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">K_fold</span>(<span class="params">self, k=<span class="number">5</span></span>):</span><br><span class="line">        <span class="comment"># GPA, Internships, Competitions, Penalties, Nationality, Salary</span></span><br><span class="line">        <span class="comment"># 最小二乘法的储存</span></span><br><span class="line">        beta_array, train_mse_array, test_mse_array = [], [], []</span><br><span class="line">        <span class="comment"># SGD的数据结构</span></span><br><span class="line">        <span class="comment"># weight_dict, bias_dict, train_loss_dict, test_loss_dict = &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">            train_data, train_label, test_data, test_label = <span class="variable language_">self</span>.pre_dataprocess(i)</span><br><span class="line">            <span class="comment"># 最小二乘法</span></span><br><span class="line">            beta, train_mse, test_mse = <span class="variable language_">self</span>.Least_Square(train_data, train_label, test_data, test_label)</span><br><span class="line"></span><br><span class="line">            beta_array.append(beta)</span><br><span class="line">            train_mse_array.append(train_mse)</span><br><span class="line">            test_mse_array.append(test_mse)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;############################################&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;my K-fold function:&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;beta: <span class="subst">&#123;np.mean(beta_array)&#125;</span> + <span class="subst">&#123;np.var(beta_array)&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;train_mse: <span class="subst">&#123;np.mean(train_mse_array)&#125;</span> + <span class="subst">&#123;np.var(train_mse_array)&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;test_mse: <span class="subst">&#123;np.mean(test_mse_array)&#125;</span> + <span class="subst">&#123;np.var(test_mse_array)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        x_labels = [<span class="string">f&#x27;Run <span class="subst">&#123;i + <span class="number">1</span>&#125;</span>&#x27;</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>)]</span><br><span class="line">        x = np.arange(<span class="built_in">len</span>(x_labels))  <span class="comment"># X 轴的位置</span></span><br><span class="line">        width = <span class="number">0.35</span>  <span class="comment"># 柱子的宽度</span></span><br><span class="line"></span><br><span class="line">        fig, ax = plt.subplots()</span><br><span class="line">        <span class="comment"># 绘制柱状图</span></span><br><span class="line"></span><br><span class="line">        train_mse_array_scaled = [value * <span class="number">1e15</span> <span class="keyword">for</span> value <span class="keyword">in</span> train_mse_array]</span><br><span class="line">        test_mse_array_scaled = [value * <span class="number">1e15</span> <span class="keyword">for</span> value <span class="keyword">in</span> test_mse_array]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 绘制训练 MSE 和测试 MSE 的柱状图</span></span><br><span class="line">        rects1 = ax.bar(x - width / <span class="number">2</span>, train_mse_array_scaled, width, label=<span class="string">&#x27;Training MSE&#x27;</span>, color=<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line">        rects2 = ax.bar(x + width / <span class="number">2</span>, test_mse_array_scaled, width, label=<span class="string">&#x27;Testing MSE&#x27;</span>, color=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        ax.set_ylabel(<span class="string">&#x27;MSE / e^-15&#x27;</span>)</span><br><span class="line">        ax.set_title(<span class="string">&#x27;Training and Testing MSE for Each Run&#x27;</span>)</span><br><span class="line">        ax.set_xticks(x)</span><br><span class="line">        ax.set_xticklabels(x_labels)</span><br><span class="line">        ax.legend()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> rect <span class="keyword">in</span> rects1:</span><br><span class="line">            height = rect.get_height()</span><br><span class="line">            ax.annotate(<span class="string">f&#x27;<span class="subst">&#123;height:<span class="number">.2</span>f&#125;</span>&#x27;</span>,</span><br><span class="line">                        xy=(rect.get_x() + rect.get_width() / <span class="number">2</span>, height),</span><br><span class="line">                        xytext=(<span class="number">0</span>, <span class="number">3</span>),  <span class="comment"># 偏移量</span></span><br><span class="line">                        textcoords=<span class="string">&quot;offset points&quot;</span>,</span><br><span class="line">                        ha=<span class="string">&#x27;center&#x27;</span>, va=<span class="string">&#x27;bottom&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> rect <span class="keyword">in</span> rects2:</span><br><span class="line">            height = rect.get_height()</span><br><span class="line">            ax.annotate(<span class="string">f&#x27;<span class="subst">&#123;height:<span class="number">.2</span>f&#125;</span>&#x27;</span>,</span><br><span class="line">                        xy=(rect.get_x() + rect.get_width() / <span class="number">2</span>, height),</span><br><span class="line">                        xytext=(<span class="number">0</span>, <span class="number">3</span>),  <span class="comment"># 偏移量</span></span><br><span class="line">                        textcoords=<span class="string">&quot;offset points&quot;</span>,</span><br><span class="line">                        ha=<span class="string">&#x27;center&#x27;</span>, va=<span class="string">&#x27;bottom&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        plt.tight_layout()</span><br><span class="line">        plt.show()</span><br><span class="line"></span><br><span class="line">        <span class="comment">####################################</span></span><br><span class="line">        <span class="comment"># sklearn库进行验证</span></span><br><span class="line">        <span class="comment">####################################</span></span><br><span class="line">        kf = KFold(n_splits=<span class="number">5</span>)</span><br><span class="line">        model = LinearRegression()</span><br><span class="line">        train_mse_list, test_mse_list = [], []</span><br><span class="line">        <span class="keyword">for</span> train_index, test_index <span class="keyword">in</span> kf.split(train_data):</span><br><span class="line">            train_data_kf, test_data_kf = train_data[train_index], train_data[test_index]</span><br><span class="line">            train_label_kf, test_label_kf = train_label[train_index], train_label[test_index]</span><br><span class="line"></span><br><span class="line">            model.fit(train_data_kf, train_label_kf)</span><br><span class="line"></span><br><span class="line">            train_pred = model.predict(train_data_kf)</span><br><span class="line">            test_pred = model.predict(test_data_kf)</span><br><span class="line"></span><br><span class="line">            train_mse = mean_squared_error(train_label_kf, train_pred)</span><br><span class="line">            test_mse = mean_squared_error(test_label_kf, test_pred)</span><br><span class="line"></span><br><span class="line">            train_mse_list.append(train_mse)</span><br><span class="line">            test_mse_list.append(test_mse)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;############################################&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;sklearn K-fold function:&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;beta: &quot;</span>, model.intercept_)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;train_mse: &quot;</span>, np.mean(train_mse_list))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;test_mse: &quot;</span>, np.mean(test_mse_list))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;############################################&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># SGD的实现</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">SGD</span>(<span class="params">self, train_data, train_label, test_data, test_label, learning_rate=<span class="number">0.001</span>, max_iter=<span class="number">100</span>, batch=<span class="number">1</span></span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化相关参数</span></span><br><span class="line">        n_samples, n_features = train_data.shape</span><br><span class="line">        weights = np.zeros(n_features)</span><br><span class="line">        bias = <span class="number">0</span></span><br><span class="line">        <span class="comment"># 保存相关的 mse 差值</span></span><br><span class="line">        train_loss_list, test_loss_list = [], []</span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(max_iter):</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, n_samples, batch):</span><br><span class="line">                X_batch = train_data[i:i + batch]</span><br><span class="line">                y_batch = train_label[i:i + batch]</span><br><span class="line"></span><br><span class="line">                y_pred = np.dot(X_batch, weights) + bias</span><br><span class="line"></span><br><span class="line">                gradient_w = -<span class="number">2</span> * np.dot(X_batch.T, (y_batch - y_pred)) / batch</span><br><span class="line">                gradient_b = -<span class="number">2</span> * np.mean(y_batch - y_pred)</span><br><span class="line"></span><br><span class="line">                weights -= learning_rate * gradient_w</span><br><span class="line">                bias -= learning_rate * gradient_b</span><br><span class="line"></span><br><span class="line">            train_loss = np.mean((train_label - (np.dot(train_data, weights) + bias)) ** <span class="number">2</span>) * <span class="number">1e11</span></span><br><span class="line">            train_loss_list.append(train_loss)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 用训练好的参数预测测试集，计算 mse</span></span><br><span class="line">        test_loss = np.mean((test_label - (np.dot(test_data, weights) + bias)) ** <span class="number">2</span>) * <span class="number">1e11</span></span><br><span class="line">        test_loss_list.append(test_loss)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># if epoch % 10 == 0:</span></span><br><span class="line">            <span class="comment">#     print(f&quot;Epoch &#123;epoch + 1&#125;/&#123;max_iter&#125;, Training Loss: &#123;train_loss:.4f&#125;, Test Loss: &#123;test_loss:.4f&#125;&quot;)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> weights, bias, train_loss_list, test_loss_list</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">mysgd</span>(<span class="params">self, k=<span class="number">5</span></span>):</span><br><span class="line">        <span class="comment"># GPA, Internships, Competitions, Penalties, Nationality, Salary</span></span><br><span class="line">        <span class="comment"># SGD的数据结构</span></span><br><span class="line">        batches = [<span class="number">1</span>, <span class="number">4</span>, <span class="number">8</span>, <span class="number">16</span>]</span><br><span class="line">        weight_dict, bias_dict, train_loss_dict, test_loss_dict = &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment"># SGD算法</span></span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> batches:</span><br><span class="line">            weight_b = []</span><br><span class="line">            bias_b = []</span><br><span class="line">            train_loss_b = []</span><br><span class="line">            test_loss_b = []</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">                train_data, train_label, test_data, test_label = <span class="variable language_">self</span>.pre_dataprocess(i)</span><br><span class="line"></span><br><span class="line">                sgd_weight, sgd_bias, train_loss_mse, test_loss_mse = <span class="variable language_">self</span>.SGD(train_data, train_label, test_data, test_label, learning_rate=<span class="number">0.001</span>, batch=batch)</span><br><span class="line"></span><br><span class="line">                weight_b.append(sgd_weight)</span><br><span class="line">                bias_b.append(sgd_bias)</span><br><span class="line">                train_loss_b.append(train_loss_mse)</span><br><span class="line">                test_loss_b.append(test_loss_mse)</span><br><span class="line"></span><br><span class="line">            weight_dict[batch] = np.mean(weight_b, axis=<span class="number">0</span>)</span><br><span class="line">            bias_dict[batch] = np.mean(bias_b, axis=<span class="number">0</span>)</span><br><span class="line">            train_loss_dict[batch] = np.mean(train_loss_b, axis=<span class="number">0</span>)</span><br><span class="line">            test_loss_dict[batch] = np.mean(test_loss_b, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        fig = plt.figure(figsize=(<span class="number">16</span>, <span class="number">12</span>))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;############################################&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;my SGD function:&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> batches:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;############################################&quot;</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;batch: &quot;</span>, batch)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;weight: &quot;</span>, weight_dict[batch])</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;bias: &quot;</span>, bias_dict[batch])</span><br><span class="line">            <span class="comment"># print(&quot;train_mse: &quot;, train_loss_dict[batch])</span></span><br><span class="line">            <span class="comment"># print(&quot;test_mse: &quot;, test_loss_dict[batch])</span></span><br><span class="line">            ax = fig.add_subplot(<span class="number">2</span>, <span class="number">2</span>, batches.index(batch) + <span class="number">1</span>)</span><br><span class="line">            ax.plot(train_loss_dict[batch], label=<span class="string">&#x27;train_mse&#x27;</span>)</span><br><span class="line">            <span class="comment"># ax.plot(test_loss_dict[batch], label=&#x27;test_mse&#x27;)</span></span><br><span class="line">            ax.set_title(<span class="string">f&#x27;Batch Size: <span class="subst">&#123;batch&#125;</span>&#x27;</span>)</span><br><span class="line">            ax.set_xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">            ax.set_ylabel(<span class="string">&#x27;MSE ^ 1e-11&#x27;</span>)</span><br><span class="line">            ax.legend()</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;############################################&quot;</span>)</span><br><span class="line">        plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 作业一中的可视化，包括数据分布和K折交叉验证</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">task1</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="variable language_">self</span>.Visualization()</span><br><span class="line">        <span class="variable language_">self</span>.K_fold()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 作业二中的SGD算法</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">task2</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="variable language_">self</span>.mysgd()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 感知机算法</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Perceptron</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, learning_rate=<span class="number">0.001</span>, max_iter=<span class="number">100</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.learning_rate = learning_rate</span><br><span class="line">        <span class="variable language_">self</span>.max_iter = max_iter</span><br><span class="line">        <span class="variable language_">self</span>.weights = <span class="literal">None</span></span><br><span class="line">        <span class="variable language_">self</span>.bias = <span class="literal">None</span></span><br><span class="line">        <span class="variable language_">self</span>.dp = dataprocess(<span class="string">&#x27;salary.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">SGD</span>(<span class="params">self, train_data, train_label, test_data, test_label, learning_rate=<span class="number">0.001</span>, max_iter=<span class="number">100</span>, batch=<span class="number">1</span></span>):</span><br><span class="line"></span><br><span class="line">        n_samples, n_features = train_data.shape</span><br><span class="line">        weights = np.zeros(n_features)</span><br><span class="line">        bias = <span class="number">0</span></span><br><span class="line">        train_loss_list, test_loss_list = [], []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(max_iter):</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, n_samples, batch):</span><br><span class="line">                X_batch = train_data[i:i + batch]</span><br><span class="line">                y_batch = train_label[i:i + batch]</span><br><span class="line"></span><br><span class="line">                y_pred = <span class="variable language_">self</span>.sigmoid(np.dot(X_batch, weights) + bias)</span><br><span class="line"></span><br><span class="line">                gradient_w = -<span class="number">2</span> * np.dot(X_batch.T, (y_batch - y_pred)) / batch</span><br><span class="line">                gradient_b = -<span class="number">2</span> * np.mean(y_batch - y_pred)</span><br><span class="line"></span><br><span class="line">                weights -= learning_rate * gradient_w</span><br><span class="line">                bias -= learning_rate * gradient_b</span><br><span class="line"></span><br><span class="line">                y_train_pred = <span class="variable language_">self</span>.sigmoid(np.dot(train_data, weights) + bias)</span><br><span class="line">                train_loss = -np.mean(</span><br><span class="line">                    train_label * np.log(y_train_pred + <span class="number">1e-9</span>) + (<span class="number">1</span> - train_label) * np.log(<span class="number">1</span> - y_train_pred + <span class="number">1e-9</span>))</span><br><span class="line">                train_loss_list.append(train_loss)</span><br><span class="line"></span><br><span class="line">            y_test_pred = <span class="variable language_">self</span>.sigmoid(np.dot(test_data, weights) + bias)</span><br><span class="line">            test_loss = -np.mean(test_label * np.log(y_test_pred + <span class="number">1e-9</span>) + (<span class="number">1</span> - test_label) * np.log(<span class="number">1</span> - y_test_pred + <span class="number">1e-9</span>))</span><br><span class="line">            test_loss_list.append(test_loss)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># if epoch % 10 == 0:</span></span><br><span class="line">            <span class="comment">#     print(f&quot;Epoch &#123;epoch + 1&#125;/&#123;max_iter&#125;, Training Loss: &#123;train_loss:.4f&#125;, Test Loss: &#123;test_loss:.4f&#125;&quot;)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> weights, bias, train_loss_list, test_loss_list</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, k=<span class="number">5</span></span>):</span><br><span class="line">        weight_list, bias_list,train_mse_list, test_mse_list = [], [], [], []</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;training with Perceptron&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">            train_data, train_label, test_data, test_label = <span class="variable language_">self</span>.dp.task3_dataprocess(i)</span><br><span class="line">            weight, bias, train_mse_error, test_mse_error = <span class="variable language_">self</span>.SGD(train_data, train_label, test_data, test_label, learning_rate=<span class="number">0.001</span>, batch=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            weight_list.append(weight)</span><br><span class="line">            bias_list.append(bias)</span><br><span class="line">            train_mse_list.append(train_mse_error)</span><br><span class="line">            test_mse_list.append(test_mse_error)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;############################################&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Perceptron function:&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;############################################&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;weight: &quot;</span>, np.mean(weight_list, axis=<span class="number">0</span>))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;bias: &quot;</span>, np.mean(bias_list, axis=<span class="number">0</span>))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;train_mse: &quot;</span>, np.mean(train_mse_list, axis=<span class="number">0</span>))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;test_mse: &quot;</span>, np.mean(test_mse_list, axis=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">        plt.plot(np.mean(train_mse_list, axis=<span class="number">1</span>), label=<span class="string">&#x27;train_mse&#x27;</span>)</span><br><span class="line">        <span class="comment"># plt.plot(np.mean(test_mse_list, axis=0), label=&#x27;test_mse&#x27;)</span></span><br><span class="line">        plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">        plt.ylabel(<span class="string">&#x27;MSE&#x27;</span>)</span><br><span class="line">        plt.legend()</span><br><span class="line">        plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LogisticRegressionSGD</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, learning_rate=<span class="number">0.001</span>, max_iter=<span class="number">100</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.learning_rate = learning_rate</span><br><span class="line">        <span class="variable language_">self</span>.max_iter = max_iter</span><br><span class="line">        <span class="variable language_">self</span>.weights = <span class="literal">None</span></span><br><span class="line">        <span class="variable language_">self</span>.bias = <span class="literal">None</span></span><br><span class="line">        <span class="variable language_">self</span>.dp = dataprocess(<span class="string">&#x27;salary.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">self, z</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">SGD</span>(<span class="params">self, train_data, train_label, test_data, test_label, learning_rate=<span class="number">0.001</span>, max_iter=<span class="number">1000</span>, batch=<span class="number">1</span></span>):</span><br><span class="line"></span><br><span class="line">        n_samples, n_features = train_data.shape</span><br><span class="line">        weights = np.zeros(n_features)</span><br><span class="line">        bias = <span class="number">0</span></span><br><span class="line">        train_loss_list, test_loss_list = [], []</span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(max_iter):</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, n_samples, batch):</span><br><span class="line">                X_batch = train_data[i:i + batch]</span><br><span class="line">                y_batch = train_label[i:i + batch]</span><br><span class="line"></span><br><span class="line">                y_pred = np.dot(X_batch, weights) + bias</span><br><span class="line">                y_pred = <span class="variable language_">self</span>.sigmoid(y_pred)</span><br><span class="line"></span><br><span class="line">                gradient_w = -<span class="number">2</span> * np.dot(X_batch.T, (y_batch - y_pred)) / batch</span><br><span class="line">                gradient_b = -<span class="number">2</span> * np.mean(y_batch - y_pred)</span><br><span class="line"></span><br><span class="line">                weights -= learning_rate * gradient_w</span><br><span class="line">                bias -= learning_rate * gradient_b</span><br><span class="line"></span><br><span class="line">            train_loss = -np.mean(y_batch * np.log(y_pred + <span class="number">1e-9</span>) + (<span class="number">1</span> - y_batch) * np.log(<span class="number">1</span> - y_pred + <span class="number">1e-9</span>))</span><br><span class="line">            train_loss_list.append(train_loss)</span><br><span class="line"></span><br><span class="line">        test_pred = <span class="variable language_">self</span>.sigmoid(np.dot(test_data, weights) + bias)</span><br><span class="line">        test_loss = -np.mean(test_label * np.log(test_pred + <span class="number">1e-9</span>) + (<span class="number">1</span> - test_label) * np.log(<span class="number">1</span> - test_pred + <span class="number">1e-9</span>))</span><br><span class="line">        test_loss_list.append(test_loss)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># if epoch % 10 == 0:</span></span><br><span class="line">            <span class="comment">#     print(f&quot;Epoch &#123;epoch + 1&#125;/&#123;max_iter&#125;, Training Loss: &#123;train_loss:.4f&#125;, Test Loss: &#123;test_loss:.4f&#125;&quot;)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> weights, bias, train_loss_list, test_loss_list</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, k=<span class="number">5</span></span>):</span><br><span class="line">        weight_list, bias_list,train_mse_list, test_mse_list = [], [], [], []</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;training Logistic Regression model...&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">            train_data, train_label, test_data, test_label = <span class="variable language_">self</span>.dp.task3_dataprocess(i)</span><br><span class="line">            weight, bias, train_mse_error, test_mse_error = <span class="variable language_">self</span>.SGD(train_data, train_label, test_data, test_label, learning_rate=<span class="number">0.01</span>, batch=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            weight_list.append(weight)</span><br><span class="line">            bias_list.append(bias)</span><br><span class="line">            train_mse_list.append(train_mse_error)</span><br><span class="line">            test_mse_list.append(test_mse_error)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(test_mse_list)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;############################################&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Logistic Regression function:&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;############################################&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;weight: &quot;</span>, np.mean(weight_list, axis=<span class="number">0</span>))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;bias: &quot;</span>, np.mean(bias_list, axis=<span class="number">0</span>))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;train_mse: &quot;</span>, np.mean(train_mse_list, axis=<span class="number">0</span>))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;test_mse: &quot;</span>, np.mean(test_mse_list, axis=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">        plt.plot(np.mean(train_mse_list, axis=<span class="number">0</span>), label=<span class="string">&#x27;train_mse&#x27;</span>)</span><br><span class="line">        <span class="comment"># plt.plot(np.mean(test_mse_list, axis=0), label=&#x27;test_mse&#x27;)</span></span><br><span class="line">        plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">        plt.ylabel(<span class="string">&#x27;MSE&#x27;</span>)</span><br><span class="line">        plt.legend()</span><br><span class="line">        plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DecisionTree</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, epsilon=<span class="number">0.1</span>, data_path=<span class="string">&#x27;salary.csv&#x27;</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.epsilon = epsilon  <span class="comment"># 停止生长的阈值</span></span><br><span class="line">        <span class="variable language_">self</span>.tree = <span class="literal">None</span></span><br><span class="line">        <span class="variable language_">self</span>.dp = dataprocess(data_path)</span><br><span class="line">        <span class="variable language_">self</span>.labels = [<span class="string">&quot;GPA&quot;</span>, <span class="string">&quot;Internships&quot;</span>, <span class="string">&quot;Competitions&quot;</span>, <span class="string">&quot;Penalties&quot;</span>, <span class="string">&quot;Australia&quot;</span>, <span class="string">&quot;U.K.&quot;</span>, <span class="string">&quot;U.S.&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_grow_tree</span>(<span class="params">self, X, y</span>):</span><br><span class="line">        num_samples = <span class="built_in">len</span>(y)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 检查样本数量</span></span><br><span class="line">        <span class="keyword">if</span> num_samples &lt; <span class="number">2</span>:  <span class="comment"># 至少需要两个样本才能分割</span></span><br><span class="line">            <span class="keyword">return</span> &#123;<span class="string">&#x27;label&#x27;</span>: np.argmax(np.bincount(y)), <span class="string">&#x27;num_samples&#x27;</span>: num_samples, <span class="string">&#x27;loss&#x27;</span>: <span class="variable language_">self</span>._cross_entropy(y)&#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(<span class="built_in">set</span>(y)) == <span class="number">1</span>:  <span class="comment"># 如果所有样本属于同一类</span></span><br><span class="line">            <span class="keyword">return</span> &#123;<span class="string">&#x27;label&#x27;</span>: y[<span class="number">0</span>], <span class="string">&#x27;num_samples&#x27;</span>: num_samples, <span class="string">&#x27;loss&#x27;</span>: <span class="variable language_">self</span>._cross_entropy(y)&#125;</span><br><span class="line"></span><br><span class="line">        probabilities = np.bincount(y) / num_samples</span><br><span class="line">        <span class="keyword">if</span> np.<span class="built_in">max</span>(probabilities) &gt;= (<span class="number">1</span> - <span class="variable language_">self</span>.epsilon):  <span class="comment"># 如果几乎纯净</span></span><br><span class="line">            <span class="keyword">return</span> &#123;<span class="string">&#x27;label&#x27;</span>: np.argmax(probabilities), <span class="string">&#x27;num_samples&#x27;</span>: num_samples, <span class="string">&#x27;loss&#x27;</span>: <span class="variable language_">self</span>._cross_entropy(y)&#125;</span><br><span class="line"></span><br><span class="line">        best_feature = <span class="variable language_">self</span>._best_feature_to_split(X, y)</span><br><span class="line">        threshold = np.median(X[:, best_feature])  <span class="comment"># 使用中位数作为阈值</span></span><br><span class="line">        left_indices = X[:, best_feature] &lt;= threshold</span><br><span class="line">        right_indices = X[:, best_feature] &gt; threshold</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 检查左右子集是否有样本</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> np.<span class="built_in">any</span>(left_indices) <span class="keyword">or</span> <span class="keyword">not</span> np.<span class="built_in">any</span>(right_indices):</span><br><span class="line">            <span class="keyword">return</span> &#123;<span class="string">&#x27;label&#x27;</span>: np.argmax(np.bincount(y)), <span class="string">&#x27;num_samples&#x27;</span>: num_samples, <span class="string">&#x27;loss&#x27;</span>: <span class="variable language_">self</span>._cross_entropy(y)&#125;</span><br><span class="line"></span><br><span class="line">        tree = &#123;</span><br><span class="line">            <span class="string">&#x27;feature_index&#x27;</span>: best_feature,</span><br><span class="line">            <span class="string">&#x27;threshold&#x27;</span>: threshold,</span><br><span class="line">            <span class="string">&#x27;children&#x27;</span>: &#123;</span><br><span class="line">                <span class="string">&#x27;left&#x27;</span>: <span class="variable language_">self</span>._grow_tree(X[left_indices], y[left_indices]),</span><br><span class="line">                <span class="string">&#x27;right&#x27;</span>: <span class="variable language_">self</span>._grow_tree(X[right_indices], y[right_indices])</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="string">&#x27;num_samples&#x27;</span>: num_samples,</span><br><span class="line">            <span class="string">&#x27;loss&#x27;</span>: <span class="variable language_">self</span>._cross_entropy(y)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> tree</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 为左右子树创建标签</span></span><br><span class="line">        left_indices = X[:, best_feature] &lt;= threshold</span><br><span class="line">        right_indices = X[:, best_feature] &gt; threshold</span><br><span class="line"></span><br><span class="line">        tree[<span class="string">&#x27;children&#x27;</span>][<span class="string">&#x27;left&#x27;</span>] = <span class="variable language_">self</span>._grow_tree(X[left_indices], y[left_indices])</span><br><span class="line">        tree[<span class="string">&#x27;children&#x27;</span>][<span class="string">&#x27;right&#x27;</span>] = <span class="variable language_">self</span>._grow_tree(X[right_indices], y[right_indices])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> tree</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_cross_entropy</span>(<span class="params">self, y</span>):</span><br><span class="line">        probabilities = np.bincount(y) / <span class="built_in">len</span>(y)</span><br><span class="line">        <span class="keyword">return</span> -np.<span class="built_in">sum</span>(probabilities[probabilities &gt; <span class="number">0</span>] * np.log(probabilities[probabilities &gt; <span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_best_feature_to_split</span>(<span class="params">self, X, y</span>):</span><br><span class="line">        num_features = X.shape[<span class="number">1</span>]</span><br><span class="line">        best_gain = -np.inf</span><br><span class="line">        best_feature = -<span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> feature <span class="keyword">in</span> <span class="built_in">range</span>(num_features):</span><br><span class="line">            gain = <span class="variable language_">self</span>._information_gain(X, y, feature)</span><br><span class="line">            <span class="keyword">if</span> gain &gt; best_gain:</span><br><span class="line">                best_gain = gain</span><br><span class="line">                best_feature = feature</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> best_feature</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_information_gain</span>(<span class="params">self, X, y, feature_index</span>):</span><br><span class="line">        total_entropy = <span class="variable language_">self</span>._cross_entropy(y)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 检查样本数量</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(y) &lt;= <span class="number">1</span>:  <span class="comment"># 至少需要两个样本才能分割</span></span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算阈值</span></span><br><span class="line">        threshold = np.median(X[:, feature_index])  <span class="comment"># 使用中位数作为阈值</span></span><br><span class="line">        left_indices = X[:, feature_index] &lt;= threshold</span><br><span class="line">        right_indices = X[:, feature_index] &gt; threshold</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 检查左右子集是否有样本</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> np.<span class="built_in">any</span>(left_indices) <span class="keyword">or</span> <span class="keyword">not</span> np.<span class="built_in">any</span>(right_indices):</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>  <span class="comment"># 如果任一子集为空，返回0</span></span><br><span class="line"></span><br><span class="line">        left_entropy = <span class="variable language_">self</span>._cross_entropy(y[left_indices])</span><br><span class="line">        right_entropy = <span class="variable language_">self</span>._cross_entropy(y[right_indices])</span><br><span class="line"></span><br><span class="line">        weighted_entropy = (</span><br><span class="line">                (np.<span class="built_in">sum</span>(left_indices) / <span class="built_in">len</span>(y)) * left_entropy +</span><br><span class="line">                (np.<span class="built_in">sum</span>(right_indices) / <span class="built_in">len</span>(y)) * right_entropy</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> total_entropy - weighted_entropy</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, X, tree</span>):</span><br><span class="line">        <span class="keyword">return</span> np.array([<span class="variable language_">self</span>._predict_sample(sample, tree) <span class="keyword">for</span> sample <span class="keyword">in</span> X])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_predict_sample</span>(<span class="params">self, sample, tree</span>):</span><br><span class="line">        <span class="keyword">if</span> tree <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> <span class="string">&#x27;label&#x27;</span> <span class="keyword">in</span> tree:</span><br><span class="line">            <span class="keyword">return</span> tree[<span class="string">&#x27;label&#x27;</span>] <span class="keyword">if</span> tree <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        feature_value = sample[tree[<span class="string">&#x27;feature_index&#x27;</span>]]</span><br><span class="line">        <span class="keyword">if</span> feature_value &lt;= tree[<span class="string">&#x27;threshold&#x27;</span>]:</span><br><span class="line">            <span class="keyword">return</span> <span class="variable language_">self</span>._predict_sample(sample, tree[<span class="string">&#x27;children&#x27;</span>][<span class="string">&#x27;left&#x27;</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="variable language_">self</span>._predict_sample(sample, tree[<span class="string">&#x27;children&#x27;</span>][<span class="string">&#x27;right&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, X, y</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>._grow_tree(X, y)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">K_fold</span>(<span class="params">self, k=<span class="number">5</span></span>):</span><br><span class="line">        test_acc = <span class="number">0</span></span><br><span class="line">        train_acc = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">            train_data, train_label, test_data, test_label = <span class="variable language_">self</span>.dp.task3_dataprocess(i)</span><br><span class="line">            tree = <span class="variable language_">self</span>.train(train_data, train_label)</span><br><span class="line">            train_pred = <span class="variable language_">self</span>.predict(train_data, tree)</span><br><span class="line">            test_pred = <span class="variable language_">self</span>.predict(test_data, tree)</span><br><span class="line">            train_acc += np.<span class="built_in">sum</span>(train_pred == train_label)</span><br><span class="line">            test_acc += np.<span class="built_in">sum</span>(test_pred == test_label)</span><br><span class="line"></span><br><span class="line">        train_accuracy = train_acc / (<span class="built_in">len</span>(train_label) * k)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Tree decision K-fold train accuracy is: <span class="subst">&#123;train_accuracy&#125;</span>&#x27;</span>)</span><br><span class="line">        test_accuracy = test_acc / (<span class="built_in">len</span>(test_label) * k)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Tree decision K-fold test accuracy is: <span class="subst">&#123;test_accuracy&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="comment"># print(f&#x27;train_data length is :&#123;len(train_label)&#125;&#x27;)</span></span><br><span class="line">        <span class="comment"># print(f&#x27;test_data length is :&#123;len(test_label)&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 可视化，需要下载 graphviz</span></span><br><span class="line">        <span class="comment"># self.visualize(tree, train_data)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">visualize</span>(<span class="params">self, tree, X</span>):</span><br><span class="line">        dot = Digraph()</span><br><span class="line">        <span class="comment"># dot.attr(ranksep=&#x27;1.5&#x27;, nodesep=&#x27;0.5&#x27;)</span></span><br><span class="line">        <span class="variable language_">self</span>._add_nodes_edges(dot, tree, X=X)</span><br><span class="line">        dot.render(<span class="string">&#x27;simple_decision_tree&#x27;</span> + <span class="built_in">str</span>(datetime.datetime.today()), <span class="built_in">format</span>=<span class="string">&#x27;png&#x27;</span>, view=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_add_nodes_edges</span>(<span class="params">self, dot, tree, parent_name=<span class="literal">None</span>, X=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="keyword">if</span> tree <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span>  <span class="comment"># 如果树为空，直接返回</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="string">&#x27;label&#x27;</span> <span class="keyword">in</span> tree:</span><br><span class="line">            <span class="comment"># print(f&quot;Label: &#123;tree[&#x27;label&#x27;]&#125;&quot;)</span></span><br><span class="line">            <span class="comment"># print(f&quot;Entropy: &#123;tree[&#x27;loss&#x27;]:.3f&#125;&quot;)</span></span><br><span class="line">            <span class="comment"># print(f&quot;Samples: &#123;tree[&#x27;num_samples&#x27;]&#125;&quot;)</span></span><br><span class="line">            <span class="keyword">return</span>  <span class="comment"># 不再添加叶节点</span></span><br><span class="line"></span><br><span class="line">        feature_index = tree[<span class="string">&#x27;feature_index&#x27;</span>]</span><br><span class="line">        threshold = tree[<span class="string">&#x27;threshold&#x27;</span>]</span><br><span class="line">        feature_name = <span class="variable language_">self</span>.labels[feature_index]</span><br><span class="line"></span><br><span class="line">        node_name = <span class="string">f&quot;Node_<span class="subst">&#123;parent_name&#125;</span>_<span class="subst">&#123;feature_index&#125;</span>&quot;</span></span><br><span class="line">        dot.node(node_name, <span class="string">f&quot;<span class="subst">&#123;feature_name&#125;</span> &lt;= <span class="subst">&#123;threshold&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> parent_name:</span><br><span class="line">            dot.edge(parent_name, node_name)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 递归处理子节点</span></span><br><span class="line">        <span class="variable language_">self</span>._add_nodes_edges(dot, tree[<span class="string">&#x27;children&#x27;</span>][<span class="string">&#x27;left&#x27;</span>], node_name, X=X)</span><br><span class="line">        <span class="variable language_">self</span>._add_nodes_edges(dot, tree[<span class="string">&#x27;children&#x27;</span>][<span class="string">&#x27;right&#x27;</span>], node_name, X=X)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sklearn_tree</span>(<span class="params">self, X, y</span>):</span><br><span class="line">        clf = DecisionTreeClassifier(criterion=<span class="string">&#x27;entropy&#x27;</span>, max_depth=<span class="number">5</span>)</span><br><span class="line">        clf.fit(X, y)</span><br><span class="line"></span><br><span class="line">        dot_data = clf.export_graphviz(clf, out_file=<span class="literal">None</span>,feature_names=<span class="variable language_">self</span>.labels)</span><br><span class="line">        dot = graphviz.Source(dot_data)</span><br><span class="line">        dot.view()</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">assignment2</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data_path</span>):</span><br><span class="line">        <span class="variable language_">self</span>.data_path = data_path</span><br><span class="line">        <span class="variable language_">self</span>.dp = dataprocess(data_path)</span><br><span class="line">        <span class="variable language_">self</span>.LR = LogisticRegressionSGD(data_path)</span><br><span class="line">        <span class="variable language_">self</span>.p = Perceptron(data_path)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 作业一</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">task1</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="variable language_">self</span>.dp.task1()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 作业二</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">task2</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="variable language_">self</span>.dp.task2()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 作业三</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">task3</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;############################################&quot;</span>)</span><br><span class="line">        <span class="variable language_">self</span>.LR.train()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;############################################&quot;</span>)</span><br><span class="line">        <span class="variable language_">self</span>.p.train()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;############################################&quot;</span>)</span><br><span class="line">        <span class="keyword">for</span> epsilon <span class="keyword">in</span> [<span class="number">0.05</span>, <span class="number">0.1</span>, <span class="number">0.2</span>]:</span><br><span class="line">            tree = DecisionTree(epsilon=epsilon)</span><br><span class="line">            tree.K_fold(<span class="number">5</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;############################################&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    data_path = <span class="string">&#x27;salary.csv&#x27;</span></span><br><span class="line"></span><br><span class="line">    homework = assignment2(data_path)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;############################################&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;PROBLEM 1		Regression on Salary&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;############################################&quot;</span>)</span><br><span class="line">    homework.task1()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;PROBLEM 2		Regression with SGD&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;############################################&quot;</span>)</span><br><span class="line">    homework.task2()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;############################################&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;PROBLEM 3		Classification on Salary&quot;</span>)</span><br><span class="line">    homework.task3()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;############################################&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1>PROBLEM 1</h1>
<h2 id="visualization">Visualization</h2>
<h3 id="salary">salary</h3>
<p>![[Pasted image 20241007152404.png]]</p>
<blockquote>
<ol>
<li>薪资是呈现一个正太分布的情况</li>
<li>大部分人的薪资水平都处于[100000， 175000]的区间</li>
</ol>
</blockquote>
<h3 id="gpa">GPA</h3>
<p>![[Pasted image 20241007152612.png]]</p>
<blockquote>
<ol>
<li>GPA与薪水呈现一个线性关系，随着 GPA 的上升，薪资呈现出一个上升的趋势。</li>
<li>GPA的分布主要集中在 [2.00， 4.00]之间</li>
</ol>
</blockquote>
<h3 id="internships">Internships</h3>
<p>![[Pasted image 20241007153013.png]]</p>
<blockquote>
<ol>
<li>Internships 是一个离散型的数据，随着 internships 的升高，可以发现薪资的上限在不断加高，下限也在不断加高，同时分布集中点也在提高</li>
</ol>
</blockquote>
<h3 id="competitions">Competitions</h3>
<p>![[Pasted image 20241007153209.png]]</p>
<blockquote>
<p>Competitions 是一个离散型的数据，随着 Competitions 的升高，可以发现薪资的上限在不断加高，下限也在不断加高，同时分布集中点也在提高</p>
</blockquote>
<h3 id="penalties">Penalties</h3>
<p>![[Pasted image 20241007153348.png]]</p>
<blockquote>
<p>Penalties 是一个离散型的数据，随着 Penalties 的升高，可以发现薪资的上限和下限在不断降低，但是受到处罚为 2.0 的人数相对较少</p>
</blockquote>
<h3 id="nationality">Nationality</h3>
<p>![[Pasted image 20241007153538.png]]</p>
<blockquote>
<p>Nationality 为离散数据，数据中 Australia, U.K.占数据的大部分，U.S. 人数明显少于其他两国</p>
</blockquote>
<h2 id="k-fold-cross-validation">K-fold cross validation</h2>
<p>![[Pasted image 20241007153827.png]]</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">最终实验数据如下：</span><br><span class="line">############################################</span><br><span class="line">my K-fold function:</span><br><span class="line">beta: [ 4.86229593e-06  1.73602268e-05  8.67408337e-06 -3.47258147e-05</span><br><span class="line"> -9.21844391e-06 -4.87899209e-06 -3.13396456e-06] ± [6.81674363e-08 4.79016568e-07 2.40329394e-07 9.61716710e-07</span><br><span class="line"> 7.41399918e-07 6.83886356e-07 6.64396429e-07]</span><br><span class="line"> </span><br><span class="line">train_mse: 5.465679839331648e-15 ± 3.2737986198218384e-16</span><br><span class="line">test_mse: 5.641269773977836e-15 ± 4.453573881649367e-16</span><br><span class="line">############################################</span><br><span class="line">sklearn K-fold function:</span><br><span class="line">beta:  [ 4.90709664e-06  1.82544487e-05  9.12638507e-06 -3.65263865e-05</span><br><span class="line"> -3.65840432e-06  9.10871654e-07  2.74753266e-06]</span><br><span class="line"> </span><br><span class="line">train_mse:  6.041639987563109e-15</span><br><span class="line">test_mse:  6.474849714869361e-15</span><br><span class="line">###########################################</span><br></pre></td></tr></table></figure>
<blockquote>
<p>可以发现，与标准库相比，数据误差较小</p>
</blockquote>
<h3 id="conclusions">Conclusions</h3>
<blockquote>
<ol>
<li>训练MSE和测试MSE均在 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>15</mn></mrow></msup></mrow><annotation encoding="application/x-tex">10^{-15}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">1</span><span class="mord mtight">5</span></span></span></span></span></span></span></span></span></span></span></span> 到  <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>16</mn></mrow></msup></mrow><annotation encoding="application/x-tex">10^{-16}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">1</span><span class="mord mtight">6</span></span></span></span></span></span></span></span></span></span></span></span> 的数量级，显示出模型对数据的拟合非常精确，几乎没有预测误差。这表明模型能够很好地捕捉数据中的趋势和关系。</li>
<li>数据中包括了多种特征（如GPA、实习经历、竞赛次数和违规记录），这些特征可能对薪资有显著影响，并且呈现线性关系。</li>
</ol>
</blockquote>
<h1>PROBLEM 2           Regression with SGD</h1>
<p>![[Pasted image 20241007160209.png]]</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">实验数据如下:</span><br><span class="line">(其中train_mse,test_mse由于plt显示精度有限，均放大了1e11倍，上图为每次迭代时候train_mse 的变化图片)</span><br><span class="line"></span><br><span class="line">############################################</span><br><span class="line">my SGD function:</span><br><span class="line">############################################</span><br><span class="line">batch:  1</span><br><span class="line">weight:  [ 4.86256567e-06  1.73605319e-05  8.67414653e-06 -3.47259825e-05</span><br><span class="line"> -4.91039945e-06 -5.71725846e-07  1.17409987e-06]</span><br><span class="line">bias:  -4.308025419954941e-06</span><br><span class="line">test_mse:  [0.00057053]</span><br><span class="line">############################################</span><br><span class="line">batch:  4</span><br><span class="line">weight:  [ 4.86068715e-06  1.73742397e-05  8.66585492e-06 -3.47046536e-05</span><br><span class="line"> -4.84013736e-06 -5.06873714e-07  9.74622112e-07]</span><br><span class="line">bias:  -4.372388966869282e-06</span><br><span class="line">test_mse:  [0.00128614]</span><br><span class="line">############################################</span><br><span class="line">batch:  8</span><br><span class="line">weight:  [ 4.86423291e-06  1.71428948e-05  8.36850732e-06 -3.43209527e-05</span><br><span class="line"> -4.63462739e-06 -2.40183117e-07  5.75862310e-07]</span><br><span class="line">bias:  -4.298948202211374e-06</span><br><span class="line">test_mse:  [0.02306401]</span><br><span class="line">############################################</span><br><span class="line">batch:  16</span><br><span class="line">weight:  [ 4.90350060e-06  1.55124551e-05  7.04386406e-06 -3.10383599e-05</span><br><span class="line"> -4.11329838e-06  3.02350357e-07  1.62895767e-07]</span><br><span class="line">bias:  -3.648052259500667e-06</span><br><span class="line">test_mse:  [0.96528455]</span><br><span class="line">############################################</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>![[Pasted image 20241007160746.png]]</p>
<blockquote>
<p>将SGD所得权重与最小二乘法的的权重进行比较，发现每个特征的权重的误差都十分的小</p>
</blockquote>
<p>![[Pasted image 20241007161822.png]]</p>
<blockquote>
<p>在batch = 1, 4 ,8 的时候与 K- Fold 的 test-mse 相差十分的近，当达到 batch = 16 时可能由于学习率，或者迭代等因素，导致误差偏高</p>
</blockquote>
<h1>PROBLEM 3           Classification on Salary</h1>
<h2 id="logistic-regression-lr">Logistic Regression (LR)</h2>
<p>![[Pasted image 20241007162255.png]]</p>
<blockquote>
<p>随着训练次数的增加，MSE 逐渐减小，最后开始收敛在 0.3 附近</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">实验数据如下：</span><br><span class="line">（train_mse 见上图）</span><br><span class="line">############################################</span><br><span class="line">Logistic Regression function:</span><br><span class="line">############################################</span><br><span class="line">weight:  [  3.44120469  11.92428137   6.2301448  -23.86514155  -2.55695149</span><br><span class="line">   0.09951324   1.63580443]</span><br><span class="line">bias:  -0.8216338251227515</span><br><span class="line">test_mse:  [0.0390241]</span><br></pre></td></tr></table></figure>
<h2 id="perceptron">Perceptron</h2>
<p>![[Pasted image 20241007163113.png]]</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">############################################</span><br><span class="line">Perceptron function:</span><br><span class="line">############################################</span><br><span class="line">weight:  [ 0.72449474  2.15297636  0.97787551 -4.20352564 -0.25714974  0.16387187</span><br><span class="line">  0.21670045]</span><br><span class="line">bias:  0.12342257835854971</span><br><span class="line">test_mse:  [0.16665284]</span><br><span class="line">############################################</span><br></pre></td></tr></table></figure>
<h2 id="decision-tree-id3">Decision Tree (ID3)</h2>
<blockquote>
<p>决策树示意图如下</p>
</blockquote>
<p>![[Pasted image 20241007163344.png]]</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">############################################</span><br><span class="line">epsilon = 0.05</span><br><span class="line">Tree decision K-fold train accuracy is: 0.9815</span><br><span class="line">Tree decision K-fold test accuracy is: 0.908</span><br><span class="line">############################################</span><br><span class="line">epsilon = 0.1</span><br><span class="line">Tree decision K-fold train accuracy is: 0.9765</span><br><span class="line">Tree decision K-fold test accuracy is: 0.91</span><br><span class="line">############################################</span><br><span class="line">epsilon = 0.2</span><br><span class="line">Tree decision K-fold train accuracy is: 0.84</span><br><span class="line">Tree decision K-fold test accuracy is: 0.84</span><br><span class="line">############################################</span><br></pre></td></tr></table></figure>
<h2 id="conclusion">Conclusion</h2>
<blockquote>
<p>从实验结果来看，逻辑回归在测试均方误差上表现出色，表明其适合于处理线性关系的分类问题。感知器在复杂数据集上表现不足，未能有效收敛。决策树则在不同的停止标准下表现出色，显示出较高的准确性，特别是在严格的停止标准下。</p>
<p>通过本实验可以看出，不同模型在相同数据集上会有不同的表现，需要我们合理选择模型</p>
</blockquote>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://iuun05.github.io">LiamRyan</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://iuun05.github.io/2024/09/07/machine-learning/">http://iuun05.github.io/2024/09/07/machine-learning/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://iuun05.github.io" target="_blank"></a>！</span></div></div><div class="tag_share"><div class="post-share"><div class="social-share" data-image="/img/20220513011040_fc477.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="prev-post pull-left" href="/2024/09/07/deep-learning-page-reading/" title="deep-learning-page-reading"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">deep-learning-page-reading</div></div></a><a class="next-post pull-right" href="/2024/09/07/deep-learning/" title="深度学习"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">深度学习</div></div></a></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info is-center"><div class="avatar-img"><img src="/img/20220513011040_fc477.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">LiamRyan</div><div class="author-info-description">一个学不会的废物</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">8</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/iuun05"><i class="fab fa-github"></i><span>关注我</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/iuun05" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">博客部分功能还未完善，后续会陆陆续续完善，请见谅</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">week 2</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#problem-1-regression-on-salary"><span class="toc-number">1.1.</span> <span class="toc-text">PROBLEM 1		Regression on Salary</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#report"><span class="toc-number">1.1.1.</span> <span class="toc-text">report</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#source-code"><span class="toc-number">1.1.2.</span> <span class="toc-text">source code</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">PROBLEM 1</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#visualization"><span class="toc-number">2.1.</span> <span class="toc-text">Visualization</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#salary"><span class="toc-number">2.1.1.</span> <span class="toc-text">salary</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#gpa"><span class="toc-number">2.1.2.</span> <span class="toc-text">GPA</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#internships"><span class="toc-number">2.1.3.</span> <span class="toc-text">Internships</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#competitions"><span class="toc-number">2.1.4.</span> <span class="toc-text">Competitions</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#penalties"><span class="toc-number">2.1.5.</span> <span class="toc-text">Penalties</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#nationality"><span class="toc-number">2.1.6.</span> <span class="toc-text">Nationality</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#k-fold-cross-validation"><span class="toc-number">2.2.</span> <span class="toc-text">K-fold cross validation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#conclusions"><span class="toc-number">2.2.1.</span> <span class="toc-text">Conclusions</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">PROBLEM 2           Regression with SGD</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">4.</span> <span class="toc-text">PROBLEM 3           Classification on Salary</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#logistic-regression-lr"><span class="toc-number">4.1.</span> <span class="toc-text">Logistic Regression (LR)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#perceptron"><span class="toc-number">4.2.</span> <span class="toc-text">Perceptron</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#decision-tree-id3"><span class="toc-number">4.3.</span> <span class="toc-text">Decision Tree (ID3)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#conclusion"><span class="toc-number">4.4.</span> <span class="toc-text">Conclusion</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/11/17/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/" title="计算机网络">计算机网络</a><time datetime="2024-11-17T06:44:10.000Z" title="发表于 2024-11-17 14:44:10">2024-11-17</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/11/09/%E5%90%8E%E7%AB%AF%E5%AD%A6%E4%B9%A0/" title="后端学习">后端学习</a><time datetime="2024-11-09T13:15:52.000Z" title="发表于 2024-11-09 21:15:52">2024-11-09</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/10/29/Golang-GC/" title="Golang_GC">Golang_GC</a><time datetime="2024-10-29T13:29:19.000Z" title="发表于 2024-10-29 21:29:19">2024-10-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/09/19/database-course/" title="database_course">database_course</a><time datetime="2024-09-19T01:13:47.000Z" title="发表于 2024-09-19 09:13:47">2024-09-19</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/09/07/deep-learning-page-reading/" title="deep-learning-page-reading">deep-learning-page-reading</a><time datetime="2024-09-06T17:57:18.000Z" title="发表于 2024-09-07 01:57:18">2024-09-07</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By LiamRyan</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.0.0-b1"></script><script src="/js/main.js?v=5.0.0-b1"></script><div class="js-pjax"><script>(() => {
  const runMermaid = (ele) => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    Array.from(ele).forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
      const mermaidID = 'mermaid-' + index
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)
      const renderMermaid = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      // mermaid v9 and v10 compatibility
      typeof renderFn === 'string' ? renderMermaid(renderFn) : renderFn.then(({svg}) => renderMermaid(svg))
    })
  }

  const codeToMermaid = () => {
    const codeMermaidEle = document.querySelectorAll('pre > code.mermaid')
    if (codeMermaidEle.length === 0) return
    
    codeMermaidEle.forEach(ele => {
      const preEle = document.createElement('pre')
      preEle.className = 'mermaid-src'
      preEle.hidden = true
      preEle.textContent = ele.textContent
      const newEle = document.createElement('div')
      newEle.className = 'mermaid-wrap'
      newEle.appendChild(preEle)
      ele.parentNode.replaceWith(newEle)
    })
  }

  const loadMermaid = () => {
    if (false) codeToMermaid()
    const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
    if ($mermaid.length === 0) return

    const runMermaidFn = () => runMermaid($mermaid)
    btf.addGlobalFn('themeChange', runMermaidFn, 'mermaid')
    window.loadMermaid ? runMermaidFn() : btf.getScript('https://cdn.jsdelivr.net/npm/mermaid@10.9.1/dist/mermaid.min.js').then(runMermaidFn)
  }
  
  btf.addGlobalFn('encrypt', loadMermaid, 'mermaid')
  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>